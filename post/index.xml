<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Posts on Carlos_Hu</title>
        <link>https://CarlossQAQ.github.io/post/</link>
        <description>Recent content in Posts on Carlos_Hu</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en</language>
        <copyright>Carloss_Hu</copyright>
        <lastBuildDate>Fri, 29 Nov 2024 00:16:47 +0800</lastBuildDate><atom:link href="https://CarlossQAQ.github.io/post/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>LLM LECTURE1</title>
        <link>https://CarlossQAQ.github.io/p/llm-lecture1/</link>
        <pubDate>Fri, 29 Nov 2024 00:16:47 +0800</pubDate>
        
        <guid>https://CarlossQAQ.github.io/p/llm-lecture1/</guid>
        <description>&lt;table style=&#34;width:100%&#34;&gt;
&lt;tr&gt;
&lt;td style=&#34;vertical-align:middle; text-align:left;&#34;&gt;
&lt;font size=&#34;2&#34;&gt;
&lt;p&gt;Supplementary code for the &lt;a href=&#34;http://mng.bz/orYv&#34;&gt;Build a Large Language Model From Scratch&lt;/a&gt; book by &lt;a href=&#34;https://sebastianraschka.com&#34;&gt;Sebastian Raschka&lt;/a&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;Code repository: &lt;a href=&#34;https://github.com/rasbt/LLMs-from-scratch&#34;&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/rasbt/LLMs-from-scratch&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/rasbt/LLMs-from-scratch&lt;/a&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/font&gt;
&lt;/td&gt;
&lt;td style=&#34;vertical-align:middle; text-align:left;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;http://mng.bz/orYv&#34;&gt;&lt;img src=&#34;https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp&#34; width=&#34;100px&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;h1 id=&#34;chapter-2-working-with-text&#34;&gt;Chapter 2: Working with Text
&lt;/h1&gt;&lt;p&gt;Packages that are being used in this notebook:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This chapter covers data preparation and sampling to get input data &amp;ldquo;ready&amp;rdquo; for the LLM
&lt;img src=&#34;https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/01.webp?timestamp=1&#34; width=&#34;500px&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;21-understanding-word-embeddings&#34;&gt;2.1 Understanding word embeddings
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;No code in this section&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;There are many forms of embeddings; we focus on text embeddings in this book
&lt;img src=&#34;https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/02.webp&#34; width=&#34;500px&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;LLMs work with embeddings in high-dimensional spaces (i.e., thousands of dimensions)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Since we can&amp;rsquo;t visualize such high-dimensional spaces (we humans think in 1, 2, or 3 dimensions), the figure below illustrates a 2-dimensional embedding space
&lt;img src=&#34;https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/03.webp&#34; width=&#34;300px&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;22-tokenizing-text&#34;&gt;2.2 Tokenizing text
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;In this section, we tokenize text, which means breaking text into smaller units, such as individual words and punctuation characters
&lt;img src=&#34;https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/04.webp&#34; width=&#34;300px&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Load raw text we want to work with&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://en.wikisource.org/wiki/The_Verdict&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;The Verdict by Edith Wharton&lt;/a&gt; is a public domain short story&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;(If you encounter an &lt;code&gt;ssl.SSLCertVerificationError&lt;/code&gt; when executing the previous code cell, it might be due to using an outdated Python version; you can find &lt;a class=&#34;link&#34; href=&#34;https://github.com/rasbt/LLMs-from-scratch/pull/403&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;more information here on GitHub&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The goal is to tokenize and embed this text for an LLM&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Let&amp;rsquo;s develop a simple tokenizer based on some simple sample text that we can then later apply to the text above&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The following regular expression will split on whitespaces&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We don&amp;rsquo;t only want to split on whitespaces but also commas and periods, so let&amp;rsquo;s modify the regular expression to do that as well&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;As we can see, this creates empty strings, let&amp;rsquo;s remove them&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;This looks pretty good, but let&amp;rsquo;s also handle other types of punctuation, such as periods, question marks, and so on&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;This is pretty good, and we are now ready to apply this tokenization to the raw text
&lt;img src=&#34;https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/05.webp&#34; width=&#34;350px&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Let&amp;rsquo;s calculate the total number of tokens&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;23-converting-tokens-into-token-ids&#34;&gt;2.3 Converting tokens into token IDs
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Next, we convert the text tokens into token IDs that we can process via embedding layers later
&lt;img src=&#34;https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/06.webp&#34; width=&#34;500px&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;From these tokens, we can now build a vocabulary that consists of all the unique tokens&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Below are the first 50 entries in this vocabulary:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Below, we illustrate the tokenization of a short sample text using a small vocabulary:
&lt;img src=&#34;https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/07.webp?123&#34; width=&#34;500px&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Putting it now all together into a tokenizer class&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;code&gt;encode&lt;/code&gt; function turns text into token IDs&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;code&gt;decode&lt;/code&gt; function turns token IDs back into text
&lt;img src=&#34;https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/08.webp?123&#34; width=&#34;500px&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We can use the tokenizer to encode (that is, tokenize) texts into integers&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;These integers can then be embedded (later) as input of/for the LLM&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We can decode the integers back into text&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;24-adding-special-context-tokens&#34;&gt;2.4 Adding special context tokens
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;It&amp;rsquo;s useful to add some &amp;ldquo;special&amp;rdquo; tokens for unknown words and to denote the end of a text
&lt;img src=&#34;https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/09.webp?123&#34; width=&#34;500px&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Some tokenizers use special tokens to help the LLM with additional context&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Some of these special tokens are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;[BOS]&lt;/code&gt; (beginning of sequence) marks the beginning of text&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;[EOS]&lt;/code&gt; (end of sequence) marks where the text ends (this is usually used to concatenate multiple unrelated texts, e.g., two different Wikipedia articles or two different books, and so on)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;[PAD]&lt;/code&gt; (padding) if we train LLMs with a batch size greater than 1 (we may include multiple texts with different lengths; with the padding token we pad the shorter texts to the longest length so that all texts have an equal length)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;[UNK]&lt;/code&gt; to represent words that are not included in the vocabulary&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Note that GPT-2 does not need any of these tokens mentioned above but only uses an &lt;code&gt;&amp;lt;|endoftext|&amp;gt;&lt;/code&gt; token to reduce complexity&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;code&gt;&amp;lt;|endoftext|&amp;gt;&lt;/code&gt; is analogous to the &lt;code&gt;[EOS]&lt;/code&gt; token mentioned above&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;GPT also uses the &lt;code&gt;&amp;lt;|endoftext|&amp;gt;&lt;/code&gt; for padding (since we typically use a mask when training on batched inputs, we would not attend padded tokens anyways, so it does not matter what these tokens are)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;GPT-2 does not use an &lt;code&gt;&amp;lt;UNK&amp;gt;&lt;/code&gt; token for out-of-vocabulary words; instead, GPT-2 uses a byte-pair encoding (BPE) tokenizer, which breaks down words into subword units which we will discuss in a later section&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We use the &lt;code&gt;&amp;lt;|endoftext|&amp;gt;&lt;/code&gt; tokens between two independent sources of text:
&lt;img src=&#34;https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/10.webp&#34; width=&#34;500px&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Let&amp;rsquo;s see what happens if we tokenize the following text:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The above produces an error because the word &amp;ldquo;Hello&amp;rdquo; is not contained in the vocabulary&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;To deal with such cases, we can add special tokens like &lt;code&gt;&amp;quot;&amp;lt;|unk|&amp;gt;&amp;quot;&lt;/code&gt; to the vocabulary to represent unknown words&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Since we are already extending the vocabulary, let&amp;rsquo;s add another token called &lt;code&gt;&amp;quot;&amp;lt;|endoftext|&amp;gt;&amp;quot;&lt;/code&gt; which is used in GPT-2 training to denote the end of a text (and it&amp;rsquo;s also used between concatenated text, like if our training datasets consists of multiple articles, books, etc.)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We also need to adjust the tokenizer accordingly so that it knows when and how to use the new &lt;code&gt;&amp;lt;unk&amp;gt;&lt;/code&gt; token
Let&amp;rsquo;s try to tokenize text with the modified tokenizer:&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;25-bytepair-encoding&#34;&gt;2.5 BytePair encoding
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;GPT-2 used BytePair encoding (BPE) as its tokenizer&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;it allows the model to break down words that aren&amp;rsquo;t in its predefined vocabulary into smaller subword units or even individual characters, enabling it to handle out-of-vocabulary words&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For instance, if GPT-2&amp;rsquo;s vocabulary doesn&amp;rsquo;t have the word &amp;ldquo;unfamiliarword,&amp;rdquo; it might tokenize it as [&amp;ldquo;unfam&amp;rdquo;, &amp;ldquo;iliar&amp;rdquo;, &amp;ldquo;word&amp;rdquo;] or some other subword breakdown, depending on its trained BPE merges&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The original BPE tokenizer can be found here: &lt;a class=&#34;link&#34; href=&#34;https://github.com/openai/gpt-2/blob/master/src/encoder.py&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/openai/gpt-2/blob/master/src/encoder.py&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In this chapter, we are using the BPE tokenizer from OpenAI&amp;rsquo;s open-source &lt;a class=&#34;link&#34; href=&#34;https://github.com/openai/tiktoken&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;tiktoken&lt;/a&gt; library, which implements its core algorithms in Rust to improve computational performance&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;I created a notebook in the &lt;a class=&#34;link&#34; href=&#34;../02_bonus_bytepair-encoder&#34; &gt;./bytepair_encoder&lt;/a&gt; that compares these two implementations side-by-side (tiktoken was about 5x faster on the sample text)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;BPE tokenizers break down unknown words into subwords and individual characters:
&lt;img src=&#34;https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/11.webp&#34; width=&#34;300px&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;26-data-sampling-with-a-sliding-window&#34;&gt;2.6 Data sampling with a sliding window
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;We train LLMs to generate one word at a time, so we want to prepare the training data accordingly where the next word in a sequence represents the target to predict:
&lt;img src=&#34;https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/12.webp&#34; width=&#34;400px&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For each text chunk, we want the inputs and targets&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Since we want the model to predict the next word, the targets are the inputs shifted by one position to the right&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;One by one, the prediction would look like as follows:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We will take care of the next-word prediction in a later chapter after we covered the attention mechanism&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For now, we implement a simple data loader that iterates over the input dataset and returns the inputs and targets shifted by one&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Install and import PyTorch (see Appendix A for installation tips)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We use a sliding window approach, changing the position by +1:&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/13.webp?123&#34; width=&#34;500px&#34;&gt;
- Create dataset and dataloader that extract chunks from the input text dataset
- Let&#39;s test the dataloader with a batch size of 1 for an LLM with a context size of 4:
- An example using stride equal to the context length (here: 4) as shown below:
&lt;img src=&#34;https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/14.webp&#34; width=&#34;500px&#34;&gt;
- We can also create batched outputs
&lt;ul&gt;
&lt;li&gt;Note that we increase the stride here so that we don&amp;rsquo;t have overlaps between the batches, since more overlap could lead to increased overfitting&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;27-creating-token-embeddings&#34;&gt;2.7 Creating token embeddings
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The data is already almost ready for an LLM&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;But lastly let us embed the tokens in a continuous vector representation using an embedding layer&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Usually, these embedding layers are part of the LLM itself and are updated (trained) during model training
&lt;img src=&#34;https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/15.webp&#34; width=&#34;400px&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Suppose we have the following four input examples with input ids 2, 3, 5, and 1 (after tokenization):&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For the sake of simplicity, suppose we have a small vocabulary of only 6 words and we want to create embeddings of size 3:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;This would result in a 6x3 weight matrix:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For those who are familiar with one-hot encoding, the embedding layer approach above is essentially just a more efficient way of implementing one-hot encoding followed by matrix multiplication in a fully-connected layer, which is described in the supplementary code in &lt;a class=&#34;link&#34; href=&#34;../03_bonus_embedding-vs-matmul&#34; &gt;./embedding_vs_matmul&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Because the embedding layer is just a more efficient implementation that is equivalent to the one-hot encoding and matrix-multiplication approach it can be seen as a neural network layer that can be optimized via backpropagation&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;To convert a token with id 3 into a 3-dimensional vector, we do the following:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Note that the above is the 4th row in the &lt;code&gt;embedding_layer&lt;/code&gt; weight matrix&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;To embed all four &lt;code&gt;input_ids&lt;/code&gt; values above, we do&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;An embedding layer is essentially a look-up operation:
&lt;img src=&#34;https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/16.webp?123&#34; width=&#34;500px&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;You may be interested in the bonus content comparing embedding layers with regular linear layers: &lt;a class=&#34;link&#34; href=&#34;../03_bonus_embedding-vs-matmul&#34; &gt;../03_bonus_embedding-vs-matmul&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;28-encoding-word-positions&#34;&gt;2.8 Encoding word positions
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Embedding layer convert IDs into identical vector representations regardless of where they are located in the input sequence:
&lt;img src=&#34;https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/17.webp&#34; width=&#34;400px&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Positional embeddings are combined with the token embedding vector to form the input embeddings for a large language model:
&lt;img src=&#34;https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/18.webp&#34; width=&#34;500px&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The BytePair encoder has a vocabulary size of 50,257:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Suppose we want to encode the input tokens into a 256-dimensional vector representation:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If we sample data from the dataloader, we embed the tokens in each batch into a 256-dimensional vector&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If we have a batch size of 8 with 4 tokens each, this results in a 8 x 4 x 256 tensor:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;GPT-2 uses absolute position embeddings, so we just create another embedding layer:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;To create the input embeddings used in an LLM, we simply add the token and the positional embeddings:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In the initial phase of the input processing workflow, the input text is segmented into separate tokens&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Following this segmentation, these tokens are transformed into token IDs based on a predefined vocabulary:
&lt;img src=&#34;https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/19.webp&#34; width=&#34;400px&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;summary-and-takeaways&#34;&gt;Summary and takeaways
&lt;/h1&gt;&lt;p&gt;See the &lt;a class=&#34;link&#34; href=&#34;./dataloader.ipynb&#34; &gt;./dataloader.ipynb&lt;/a&gt; code notebook, which is a concise version of the data loader that we implemented in this chapter and will need for training the GPT model in upcoming chapters.&lt;/p&gt;
&lt;p&gt;See &lt;a class=&#34;link&#34; href=&#34;./exercise-solutions.ipynb&#34; &gt;./exercise-solutions.ipynb&lt;/a&gt; for the exercise solutions.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Math Typesetting</title>
        <link>https://CarlossQAQ.github.io/p/math-typesetting/</link>
        <pubDate>Fri, 08 Mar 2019 00:00:00 +0000</pubDate>
        
        <guid>https://CarlossQAQ.github.io/p/math-typesetting/</guid>
        <description>&lt;p&gt;Mathematical notation in a Hugo project can be enabled by using third party JavaScript libraries.&lt;/p&gt;
&lt;p&gt;In this example we will be using &lt;a class=&#34;link&#34; href=&#34;https://katex.org/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;KaTeX&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create a partial under &lt;code&gt;/layouts/partials/math.html&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Within this partial reference the &lt;a class=&#34;link&#34; href=&#34;https://katex.org/docs/autorender.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Auto-render Extension&lt;/a&gt; or host these scripts locally.&lt;/li&gt;
&lt;li&gt;Include the partial in your templates like so:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;{{&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; or .Params.math .Site.Params.math &lt;span class=&#34;o&#34;&gt;}}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;{{&lt;/span&gt; partial &lt;span class=&#34;s2&#34;&gt;&amp;#34;math.html&amp;#34;&lt;/span&gt; . &lt;span class=&#34;o&#34;&gt;}}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;{{&lt;/span&gt; end &lt;span class=&#34;o&#34;&gt;}}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;To enable KaTeX globally set the parameter &lt;code&gt;math&lt;/code&gt; to &lt;code&gt;true&lt;/code&gt; in a project&amp;rsquo;s configuration&lt;/li&gt;
&lt;li&gt;To enable KaTeX on a per page basis include the parameter &lt;code&gt;math: true&lt;/code&gt; in content files&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Use the online reference of &lt;a class=&#34;link&#34; href=&#34;https://katex.org/docs/supported.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Supported TeX Functions&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;examples&#34;&gt;Examples
&lt;/h3&gt;&lt;p&gt;Inline math: $\varphi = \dfrac{1+\sqrt5}{2}= 1.6180339887…$&lt;/p&gt;
&lt;p&gt;Block math:
$$
\varphi = 1+\frac{1} {1+\frac{1} {1+\frac{1} {1+\cdots} } }
$$&lt;/p&gt;</description>
        </item>
        
    </channel>
</rss>

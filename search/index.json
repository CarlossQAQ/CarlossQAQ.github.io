[{"content":" Supplementary code for the Build a Large Language Model From Scratch book by Sebastian Raschka\nCode repository: https://github.com/rasbt/LLMs-from-scratch\nChapter 2: Working with Text Packages that are being used in this notebook:\nThis chapter covers data preparation and sampling to get input data \u0026ldquo;ready\u0026rdquo; for the LLM 2.1 Understanding word embeddings No code in this section\nThere are many forms of embeddings; we focus on text embeddings in this book LLMs work with embeddings in high-dimensional spaces (i.e., thousands of dimensions)\nSince we can\u0026rsquo;t visualize such high-dimensional spaces (we humans think in 1, 2, or 3 dimensions), the figure below illustrates a 2-dimensional embedding space 2.2 Tokenizing text In this section, we tokenize text, which means breaking text into smaller units, such as individual words and punctuation characters Load raw text we want to work with\nThe Verdict by Edith Wharton is a public domain short story\n(If you encounter an ssl.SSLCertVerificationError when executing the previous code cell, it might be due to using an outdated Python version; you can find more information here on GitHub)\nThe goal is to tokenize and embed this text for an LLM\nLet\u0026rsquo;s develop a simple tokenizer based on some simple sample text that we can then later apply to the text above\nThe following regular expression will split on whitespaces\nWe don\u0026rsquo;t only want to split on whitespaces but also commas and periods, so let\u0026rsquo;s modify the regular expression to do that as well\nAs we can see, this creates empty strings, let\u0026rsquo;s remove them\nThis looks pretty good, but let\u0026rsquo;s also handle other types of punctuation, such as periods, question marks, and so on\nThis is pretty good, and we are now ready to apply this tokenization to the raw text Let\u0026rsquo;s calculate the total number of tokens\n2.3 Converting tokens into token IDs Next, we convert the text tokens into token IDs that we can process via embedding layers later From these tokens, we can now build a vocabulary that consists of all the unique tokens\nBelow are the first 50 entries in this vocabulary:\nBelow, we illustrate the tokenization of a short sample text using a small vocabulary: Putting it now all together into a tokenizer class\nThe encode function turns text into token IDs\nThe decode function turns token IDs back into text We can use the tokenizer to encode (that is, tokenize) texts into integers\nThese integers can then be embedded (later) as input of/for the LLM\nWe can decode the integers back into text\n2.4 Adding special context tokens It\u0026rsquo;s useful to add some \u0026ldquo;special\u0026rdquo; tokens for unknown words and to denote the end of a text Some tokenizers use special tokens to help the LLM with additional context\nSome of these special tokens are\n[BOS] (beginning of sequence) marks the beginning of text\n[EOS] (end of sequence) marks where the text ends (this is usually used to concatenate multiple unrelated texts, e.g., two different Wikipedia articles or two different books, and so on)\n[PAD] (padding) if we train LLMs with a batch size greater than 1 (we may include multiple texts with different lengths; with the padding token we pad the shorter texts to the longest length so that all texts have an equal length)\n[UNK] to represent words that are not included in the vocabulary\nNote that GPT-2 does not need any of these tokens mentioned above but only uses an \u0026lt;|endoftext|\u0026gt; token to reduce complexity\nThe \u0026lt;|endoftext|\u0026gt; is analogous to the [EOS] token mentioned above\nGPT also uses the \u0026lt;|endoftext|\u0026gt; for padding (since we typically use a mask when training on batched inputs, we would not attend padded tokens anyways, so it does not matter what these tokens are)\nGPT-2 does not use an \u0026lt;UNK\u0026gt; token for out-of-vocabulary words; instead, GPT-2 uses a byte-pair encoding (BPE) tokenizer, which breaks down words into subword units which we will discuss in a later section\nWe use the \u0026lt;|endoftext|\u0026gt; tokens between two independent sources of text: Let\u0026rsquo;s see what happens if we tokenize the following text:\nThe above produces an error because the word \u0026ldquo;Hello\u0026rdquo; is not contained in the vocabulary\nTo deal with such cases, we can add special tokens like \u0026quot;\u0026lt;|unk|\u0026gt;\u0026quot; to the vocabulary to represent unknown words\nSince we are already extending the vocabulary, let\u0026rsquo;s add another token called \u0026quot;\u0026lt;|endoftext|\u0026gt;\u0026quot; which is used in GPT-2 training to denote the end of a text (and it\u0026rsquo;s also used between concatenated text, like if our training datasets consists of multiple articles, books, etc.)\nWe also need to adjust the tokenizer accordingly so that it knows when and how to use the new \u0026lt;unk\u0026gt; token Let\u0026rsquo;s try to tokenize text with the modified tokenizer:\n2.5 BytePair encoding GPT-2 used BytePair encoding (BPE) as its tokenizer\nit allows the model to break down words that aren\u0026rsquo;t in its predefined vocabulary into smaller subword units or even individual characters, enabling it to handle out-of-vocabulary words\nFor instance, if GPT-2\u0026rsquo;s vocabulary doesn\u0026rsquo;t have the word \u0026ldquo;unfamiliarword,\u0026rdquo; it might tokenize it as [\u0026ldquo;unfam\u0026rdquo;, \u0026ldquo;iliar\u0026rdquo;, \u0026ldquo;word\u0026rdquo;] or some other subword breakdown, depending on its trained BPE merges\nThe original BPE tokenizer can be found here: https://github.com/openai/gpt-2/blob/master/src/encoder.py\nIn this chapter, we are using the BPE tokenizer from OpenAI\u0026rsquo;s open-source tiktoken library, which implements its core algorithms in Rust to improve computational performance\nI created a notebook in the ./bytepair_encoder that compares these two implementations side-by-side (tiktoken was about 5x faster on the sample text)\nBPE tokenizers break down unknown words into subwords and individual characters: 2.6 Data sampling with a sliding window We train LLMs to generate one word at a time, so we want to prepare the training data accordingly where the next word in a sequence represents the target to predict: For each text chunk, we want the inputs and targets\nSince we want the model to predict the next word, the targets are the inputs shifted by one position to the right\nOne by one, the prediction would look like as follows:\nWe will take care of the next-word prediction in a later chapter after we covered the attention mechanism\nFor now, we implement a simple data loader that iterates over the input dataset and returns the inputs and targets shifted by one\nInstall and import PyTorch (see Appendix A for installation tips)\nWe use a sliding window approach, changing the position by +1:\n- Create dataset and dataloader that extract chunks from the input text dataset - Let's test the dataloader with a batch size of 1 for an LLM with a context size of 4: - An example using stride equal to the context length (here: 4) as shown below: - We can also create batched outputs Note that we increase the stride here so that we don\u0026rsquo;t have overlaps between the batches, since more overlap could lead to increased overfitting 2.7 Creating token embeddings The data is already almost ready for an LLM\nBut lastly let us embed the tokens in a continuous vector representation using an embedding layer\nUsually, these embedding layers are part of the LLM itself and are updated (trained) during model training Suppose we have the following four input examples with input ids 2, 3, 5, and 1 (after tokenization):\nFor the sake of simplicity, suppose we have a small vocabulary of only 6 words and we want to create embeddings of size 3:\nThis would result in a 6x3 weight matrix:\nFor those who are familiar with one-hot encoding, the embedding layer approach above is essentially just a more efficient way of implementing one-hot encoding followed by matrix multiplication in a fully-connected layer, which is described in the supplementary code in ./embedding_vs_matmul\nBecause the embedding layer is just a more efficient implementation that is equivalent to the one-hot encoding and matrix-multiplication approach it can be seen as a neural network layer that can be optimized via backpropagation\nTo convert a token with id 3 into a 3-dimensional vector, we do the following:\nNote that the above is the 4th row in the embedding_layer weight matrix\nTo embed all four input_ids values above, we do\nAn embedding layer is essentially a look-up operation: You may be interested in the bonus content comparing embedding layers with regular linear layers: ../03_bonus_embedding-vs-matmul\n2.8 Encoding word positions Embedding layer convert IDs into identical vector representations regardless of where they are located in the input sequence: Positional embeddings are combined with the token embedding vector to form the input embeddings for a large language model: The BytePair encoder has a vocabulary size of 50,257:\nSuppose we want to encode the input tokens into a 256-dimensional vector representation:\nIf we sample data from the dataloader, we embed the tokens in each batch into a 256-dimensional vector\nIf we have a batch size of 8 with 4 tokens each, this results in a 8 x 4 x 256 tensor:\nGPT-2 uses absolute position embeddings, so we just create another embedding layer:\nTo create the input embeddings used in an LLM, we simply add the token and the positional embeddings:\nIn the initial phase of the input processing workflow, the input text is segmented into separate tokens\nFollowing this segmentation, these tokens are transformed into token IDs based on a predefined vocabulary: Summary and takeaways See the ./dataloader.ipynb code notebook, which is a concise version of the data loader that we implemented in this chapter and will need for training the GPT model in upcoming chapters.\nSee ./exercise-solutions.ipynb for the exercise solutions.\n","date":"2024-11-29T00:16:47+08:00","permalink":"https://CarlossQAQ.github.io/p/llm-lecture1/","title":"LLM LECTURE1"},{"content":"Mathematical notation in a Hugo project can be enabled by using third party JavaScript libraries.\nIn this example we will be using KaTeX\nCreate a partial under /layouts/partials/math.html Within this partial reference the Auto-render Extension or host these scripts locally. Include the partial in your templates like so: 1 2 3 {{ if or .Params.math .Site.Params.math }} {{ partial \u0026#34;math.html\u0026#34; . }} {{ end }} To enable KaTeX globally set the parameter math to true in a project\u0026rsquo;s configuration To enable KaTeX on a per page basis include the parameter math: true in content files Note: Use the online reference of Supported TeX Functions\nExamples Inline math: $\\varphi = \\dfrac{1+\\sqrt5}{2}= 1.6180339887…$\nBlock math: $$ \\varphi = 1+\\frac{1} {1+\\frac{1} {1+\\frac{1} {1+\\cdots} } } $$\n","date":"2019-03-08T00:00:00Z","permalink":"https://CarlossQAQ.github.io/p/math-typesetting/","title":"Math Typesetting"}]
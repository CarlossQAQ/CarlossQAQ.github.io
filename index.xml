<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Carlos_Hu</title>
        <link>https://CarlossQAQ.github.io/</link>
        <description>Recent content on Carlos_Hu</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en</language>
        <copyright>Carloss_Hu</copyright>
        <lastBuildDate>Fri, 29 Nov 2024 14:40:58 +0800</lastBuildDate><atom:link href="https://CarlossQAQ.github.io/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Disease detection program logbook(continuously_updated)</title>
        <link>https://CarlossQAQ.github.io/p/disease-detection-program-logbookcontinuously_updated/</link>
        <pubDate>Fri, 29 Nov 2024 14:40:58 +0800</pubDate>
        
        <guid>https://CarlossQAQ.github.io/p/disease-detection-program-logbookcontinuously_updated/</guid>
        <description>&lt;h1 id=&#34;disease_detection-project-log&#34;&gt;Disease_detection Project Log
&lt;/h1&gt;&lt;h2 id=&#34;1113---project-kickoff&#34;&gt;11.13 - Project Kickoff
&lt;/h2&gt;&lt;p&gt;The project was officially launched under the leadership of Carlos, with Ken serving as the advisor. Team members include Medicine, Quinn, Gina, and Keith. The goal is to detect disease-related terms and enhance skills in natural language processing and deep learning.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;1124---keyword-assignment-and-tool-selection&#34;&gt;11.24 - Keyword Assignment and Tool Selection
&lt;/h2&gt;&lt;p&gt;The team finalized the keyword assignments, with each member selecting specific keywords for the project. The selections are organized as follows:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Team Member&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Keywords&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Carlos&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;- &lt;strong&gt;Down syndrome (唐氏综合征)&lt;/strong&gt;&lt;br&gt;- &lt;strong&gt;Klinefelter syndrome (超雄综合征)&lt;/strong&gt;&lt;br&gt;- &lt;strong&gt;Schizophrenia (精神分裂)&lt;/strong&gt;&lt;br&gt;- &lt;strong&gt;Bipolar disorder (躁郁症)&lt;/strong&gt;&lt;br&gt;- &lt;strong&gt;Poliomyelitis (小儿麻痹症)&lt;/strong&gt;&lt;br&gt;- &lt;strong&gt;Cerebral palsy (脑瘫)&lt;/strong&gt;&lt;br&gt;- &lt;strong&gt;Fever (发热)&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Quinn&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;- &lt;strong&gt;Hyperthyroidism (甲亢)&lt;/strong&gt;&lt;br&gt;- &lt;strong&gt;Hypertension (高血压)&lt;/strong&gt;&lt;br&gt;- &lt;strong&gt;Alzheimer&amp;rsquo;s disease (老年痴呆)&lt;/strong&gt;&lt;br&gt;- &lt;strong&gt;Stroke (中风)&lt;/strong&gt;&lt;br&gt;- &lt;strong&gt;Headache (头痛/头疼)&lt;/strong&gt;&lt;br&gt;- &lt;strong&gt;Epilepsy (羊癫疯)&lt;/strong&gt;&lt;br&gt;- &lt;strong&gt;Constipation (便秘)&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Medicine&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;- &lt;strong&gt;Internal heat (上火)&lt;/strong&gt;&lt;br&gt;- &lt;strong&gt;Cerebral infarction and thrombosis (脑梗和脑血栓)&lt;/strong&gt;&lt;br&gt;- &lt;strong&gt;Cancer (癌症)&lt;/strong&gt;&lt;br&gt;- &lt;strong&gt;Vegetative state (植物人)&lt;/strong&gt;&lt;br&gt;- &lt;strong&gt;Bipolar affective disorder (双向情感障碍)&lt;/strong&gt;&lt;br&gt;- &lt;strong&gt;Terminal illness (绝症)&lt;/strong&gt;&lt;br&gt;- &lt;strong&gt;PTSD (创伤后应激障碍)&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Ken&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;- &lt;strong&gt;Cataract (白内障)&lt;/strong&gt;&lt;br&gt;- &lt;strong&gt;Halitosis (口臭)&lt;/strong&gt;&lt;br&gt;- &lt;strong&gt;Psychosis (精神病)&lt;/strong&gt;&lt;br&gt;- &lt;strong&gt;Allergy (过敏)&lt;/strong&gt;&lt;br&gt;- &lt;strong&gt;Vomiting (呕吐)&lt;/strong&gt;&lt;br&gt;- &lt;strong&gt;Stomachache (胃痛)&lt;/strong&gt;&lt;br&gt;- &lt;strong&gt;Muscle cramp (抽筋)&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Gina&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;- &lt;strong&gt;Dizziness (头晕)&lt;/strong&gt;&lt;br&gt;- &lt;strong&gt;Asthma (哮喘)&lt;/strong&gt;&lt;br&gt;- &lt;strong&gt;Amnesia (失忆)&lt;/strong&gt;&lt;br&gt;- &lt;strong&gt;Forgetfulness (健忘)&lt;/strong&gt;&lt;br&gt;- &lt;strong&gt;Alzheimer&amp;rsquo;s (阿兹海默)&lt;/strong&gt;&lt;br&gt;- &lt;strong&gt;Dementia (痴呆)&lt;/strong&gt;&lt;br&gt;- &lt;strong&gt;Parkinson&amp;rsquo;s disease (帕金森)&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Keith&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;- &lt;strong&gt;Obsessive-compulsive disorder (OCD, 强迫症)&lt;/strong&gt;&lt;br&gt;- &lt;strong&gt;Rabies (狂犬病)&lt;/strong&gt;&lt;br&gt;- &lt;strong&gt;Dissociative identity disorder (人格分裂)&lt;/strong&gt;&lt;br&gt;- &lt;strong&gt;Kidney deficiency (肾亏)&lt;/strong&gt;&lt;br&gt;- &lt;strong&gt;Erectile dysfunction (阳痿)&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;1127---second-team-meeting&#34;&gt;11.27 - Second Team Meeting
&lt;/h2&gt;&lt;p&gt;Key issues were identified and discussed during the second meeting:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Classification of Context Meaning&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Original meaning, reflects physical condition (Label: 0).&lt;/li&gt;
&lt;li&gt;Original meaning, does not reflect physical condition (Label: 1).&lt;/li&gt;
&lt;li&gt;Sarcasm or figurative meaning (Label: 2).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Keyword Filtering&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Should certain keywords be removed?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Missing Attached Images&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How to address the issue of missing attached images in Weibo posts?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Dataset Balance&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Examine the dataset to determine if it is balanced. If imbalanced, increase data for underrepresented labels.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>Marcov_chain_prediction_algorithm</title>
        <link>https://CarlossQAQ.github.io/p/marcov_chain_prediction_algorithm/</link>
        <pubDate>Fri, 29 Nov 2024 11:44:31 +0800</pubDate>
        
        <guid>https://CarlossQAQ.github.io/p/marcov_chain_prediction_algorithm/</guid>
        <description>&lt;h2 id=&#34;what-is-a-markov-chain&#34;&gt;What is a Markov Chain?
&lt;/h2&gt;&lt;p&gt;A &lt;strong&gt;Markov Chain&lt;/strong&gt; is a mathematical model describing transitions between states based on probabilities. Its defining property is that the &lt;strong&gt;future state depends only on the current state&lt;/strong&gt;, not on past states.&lt;/p&gt;
&lt;h3 id=&#34;markov-property&#34;&gt;Markov Property
&lt;/h3&gt;&lt;p&gt;The Markov Property is expressed mathematically as:&lt;/p&gt;
&lt;p&gt;$$
P(X_{t+1} = s_j | X_t = s_i, X_{t-1}, &amp;hellip;, X_0) = P(X_{t+1} = s_j | X_t = s_i)
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;state-transition-probability-matrix&#34;&gt;State Transition Probability Matrix
&lt;/h2&gt;&lt;p&gt;The state transitions in a Markov Chain are described by a &lt;strong&gt;state transition probability matrix&lt;/strong&gt;, ( P ):&lt;/p&gt;
&lt;p&gt;$$
P =
\begin{bmatrix}
0.7 &amp;amp; 0.2 &amp;amp; 0.1 \
0.3 &amp;amp; 0.4 &amp;amp; 0.3 \
0.2 &amp;amp; 0.3 &amp;amp; 0.5
\end{bmatrix}
$$&lt;/p&gt;
&lt;p&gt;Each entry ( P(s_j|s_i) ) represents the probability of transitioning from state ( s_i ) to state ( s_j ). The rows of the matrix must sum to 1.&lt;/p&gt;
&lt;h3 id=&#34;visual-representation&#34;&gt;Visual Representation
&lt;/h3&gt;&lt;p&gt;&lt;img src=&#34;https://CarlossQAQ.github.io/p/marcov_chain_prediction_algorithm/state_transition_matrix.png&#34;
	width=&#34;954&#34;
	height=&#34;800&#34;
	srcset=&#34;https://CarlossQAQ.github.io/p/marcov_chain_prediction_algorithm/state_transition_matrix_hu5722981567273431262.png 480w, https://CarlossQAQ.github.io/p/marcov_chain_prediction_algorithm/state_transition_matrix_hu7721517031506615268.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;State Transition Matrix&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;119&#34;
		data-flex-basis=&#34;286px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://CarlossQAQ.github.io/p/marcov_chain_prediction_algorithm/state_transition_diagram.png&#34;
	width=&#34;1600&#34;
	height=&#34;1200&#34;
	srcset=&#34;https://CarlossQAQ.github.io/p/marcov_chain_prediction_algorithm/state_transition_diagram_hu6959064839851416965.png 480w, https://CarlossQAQ.github.io/p/marcov_chain_prediction_algorithm/state_transition_diagram_hu17490209543252086977.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;State Transition Diagram&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;133&#34;
		data-flex-basis=&#34;320px&#34;
	
&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;initial-state-distribution&#34;&gt;Initial State Distribution
&lt;/h2&gt;&lt;p&gt;The &lt;strong&gt;initial state distribution&lt;/strong&gt; is represented as a vector ( \pi ). For example:&lt;/p&gt;
&lt;p&gt;$$
\pi^{(0)} = [1.0, 0.0, 0.0]
$$&lt;/p&gt;
&lt;p&gt;This means the system starts entirely in state ( s_1 ) (eating out).&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;prediction-formula&#34;&gt;Prediction Formula
&lt;/h2&gt;&lt;p&gt;To predict the state distribution at the next time step, use the formula:&lt;/p&gt;
&lt;p&gt;$$
\pi^{(t+1)} = \pi^{(t)} \cdot P
$$&lt;/p&gt;
&lt;p&gt;Repeat this process iteratively to predict the distribution over multiple steps.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;practical-example-deciding-daily-activities&#34;&gt;Practical Example: Deciding Daily Activities
&lt;/h2&gt;&lt;p&gt;Imagine you’re deciding what to do tomorrow:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Eat out (( s_1 )).&lt;/li&gt;
&lt;li&gt;Go for a walk (( s_2 )).&lt;/li&gt;
&lt;li&gt;Grab coffee (( s_3 )).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Based on your habits:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If you eat out today, there’s a 70% chance you’ll eat out again tomorrow, 20% chance for a walk, and 10% chance for coffee.&lt;/li&gt;
&lt;li&gt;Similar transition probabilities apply for other states.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The state transition matrix ( P ) and initial state distribution ( \pi ) are as shown earlier. Let’s predict your activity distribution over the next 10 days.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;python-implementation&#34;&gt;Python Implementation
&lt;/h2&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;16
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;17
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;18
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;19
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;20
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;21
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;22
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;23
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;24
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;25
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;26
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;numpy&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;np&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Define the state transition matrix&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;P&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;array&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.7&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;0.2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;0.1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;0.4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;0.3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;0.3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;0.5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Define the initial state (starting with eating out)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;initial_state&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;array&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;1.0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;0.0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;0.0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Function to predict future states&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;predict_markov_chain&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;P&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;initial_state&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;steps&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;state&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;initial_state&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;predictions&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;step&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;range&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;steps&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;state&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;state&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;P&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;predictions&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;append&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;state&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Step &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;step&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;state&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;predictions&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Run the prediction for 10 steps&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;steps&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;predicted_states&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;predict_markov_chain&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;P&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;initial_state&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;steps&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Predicted State Distribution After 10 Steps:&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;predicted_states&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;hr&gt;
&lt;h2 id=&#34;example-output&#34;&gt;Example Output
&lt;/h2&gt;&lt;p&gt;Running the above code produces the following output:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Step 1: [0.7 0.2 0.1]
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Step 2: [0.58 0.28 0.14]
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Step 3: [0.494 0.318 0.188]
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;...
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Step 10: [0.4 0.3 0.3]
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;By the 10th day, the probabilities stabilize at ( [0.4, 0.3, 0.3] ), indicating equal chances for eating out, walking, or grabbing coffee.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion
&lt;/h2&gt;&lt;p&gt;Markov Chains are versatile and easy to implement. Whether predicting daily decisions, modeling weather patterns, or generating text, they provide a powerful framework for understanding probabilistic systems.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>LLM LECTURE1</title>
        <link>https://CarlossQAQ.github.io/p/llm-lecture1/</link>
        <pubDate>Fri, 29 Nov 2024 00:16:47 +0800</pubDate>
        
        <guid>https://CarlossQAQ.github.io/p/llm-lecture1/</guid>
        <description>&lt;table style=&#34;width:100%&#34;&gt;
&lt;tr&gt;
&lt;td style=&#34;vertical-align:middle; text-align:left;&#34;&gt;
&lt;font size=&#34;2&#34;&gt;
&lt;p&gt;Supplementary code for the &lt;a href=&#34;http://mng.bz/orYv&#34;&gt;Build a Large Language Model From Scratch&lt;/a&gt; book by &lt;a href=&#34;https://sebastianraschka.com&#34;&gt;Sebastian Raschka&lt;/a&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;Code repository: &lt;a href=&#34;https://github.com/rasbt/LLMs-from-scratch&#34;&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/rasbt/LLMs-from-scratch&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/rasbt/LLMs-from-scratch&lt;/a&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/font&gt;
&lt;/td&gt;
&lt;td style=&#34;vertical-align:middle; text-align:left;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;http://mng.bz/orYv&#34;&gt;&lt;img src=&#34;https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp&#34; width=&#34;100px&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;h1 id=&#34;chapter-2-working-with-text&#34;&gt;Chapter 2: Working with Text
&lt;/h1&gt;&lt;p&gt;Packages that are being used in this notebook:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This chapter covers data preparation and sampling to get input data &amp;ldquo;ready&amp;rdquo; for the LLM
&lt;img src=&#34;https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/01.webp?timestamp=1&#34; width=&#34;500px&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;21-understanding-word-embeddings&#34;&gt;2.1 Understanding word embeddings
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;No code in this section&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;There are many forms of embeddings; we focus on text embeddings in this book
&lt;img src=&#34;https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/02.webp&#34; width=&#34;500px&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;LLMs work with embeddings in high-dimensional spaces (i.e., thousands of dimensions)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Since we can&amp;rsquo;t visualize such high-dimensional spaces (we humans think in 1, 2, or 3 dimensions), the figure below illustrates a 2-dimensional embedding space
&lt;img src=&#34;https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/03.webp&#34; width=&#34;300px&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;22-tokenizing-text&#34;&gt;2.2 Tokenizing text
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;In this section, we tokenize text, which means breaking text into smaller units, such as individual words and punctuation characters
&lt;img src=&#34;https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/04.webp&#34; width=&#34;300px&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Load raw text we want to work with&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://en.wikisource.org/wiki/The_Verdict&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;The Verdict by Edith Wharton&lt;/a&gt; is a public domain short story&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;(If you encounter an &lt;code&gt;ssl.SSLCertVerificationError&lt;/code&gt; when executing the previous code cell, it might be due to using an outdated Python version; you can find &lt;a class=&#34;link&#34; href=&#34;https://github.com/rasbt/LLMs-from-scratch/pull/403&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;more information here on GitHub&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The goal is to tokenize and embed this text for an LLM&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Let&amp;rsquo;s develop a simple tokenizer based on some simple sample text that we can then later apply to the text above&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The following regular expression will split on whitespaces&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We don&amp;rsquo;t only want to split on whitespaces but also commas and periods, so let&amp;rsquo;s modify the regular expression to do that as well&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;As we can see, this creates empty strings, let&amp;rsquo;s remove them&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;This looks pretty good, but let&amp;rsquo;s also handle other types of punctuation, such as periods, question marks, and so on&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;This is pretty good, and we are now ready to apply this tokenization to the raw text
&lt;img src=&#34;https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/05.webp&#34; width=&#34;350px&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Let&amp;rsquo;s calculate the total number of tokens&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;23-converting-tokens-into-token-ids&#34;&gt;2.3 Converting tokens into token IDs
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Next, we convert the text tokens into token IDs that we can process via embedding layers later
&lt;img src=&#34;https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/06.webp&#34; width=&#34;500px&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;From these tokens, we can now build a vocabulary that consists of all the unique tokens&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Below are the first 50 entries in this vocabulary:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Below, we illustrate the tokenization of a short sample text using a small vocabulary:
&lt;img src=&#34;https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/07.webp?123&#34; width=&#34;500px&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Putting it now all together into a tokenizer class&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;code&gt;encode&lt;/code&gt; function turns text into token IDs&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;code&gt;decode&lt;/code&gt; function turns token IDs back into text
&lt;img src=&#34;https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/08.webp?123&#34; width=&#34;500px&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We can use the tokenizer to encode (that is, tokenize) texts into integers&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;These integers can then be embedded (later) as input of/for the LLM&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We can decode the integers back into text&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;24-adding-special-context-tokens&#34;&gt;2.4 Adding special context tokens
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;It&amp;rsquo;s useful to add some &amp;ldquo;special&amp;rdquo; tokens for unknown words and to denote the end of a text
&lt;img src=&#34;https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/09.webp?123&#34; width=&#34;500px&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Some tokenizers use special tokens to help the LLM with additional context&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Some of these special tokens are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;[BOS]&lt;/code&gt; (beginning of sequence) marks the beginning of text&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;[EOS]&lt;/code&gt; (end of sequence) marks where the text ends (this is usually used to concatenate multiple unrelated texts, e.g., two different Wikipedia articles or two different books, and so on)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;[PAD]&lt;/code&gt; (padding) if we train LLMs with a batch size greater than 1 (we may include multiple texts with different lengths; with the padding token we pad the shorter texts to the longest length so that all texts have an equal length)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;[UNK]&lt;/code&gt; to represent words that are not included in the vocabulary&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Note that GPT-2 does not need any of these tokens mentioned above but only uses an &lt;code&gt;&amp;lt;|endoftext|&amp;gt;&lt;/code&gt; token to reduce complexity&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;code&gt;&amp;lt;|endoftext|&amp;gt;&lt;/code&gt; is analogous to the &lt;code&gt;[EOS]&lt;/code&gt; token mentioned above&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;GPT also uses the &lt;code&gt;&amp;lt;|endoftext|&amp;gt;&lt;/code&gt; for padding (since we typically use a mask when training on batched inputs, we would not attend padded tokens anyways, so it does not matter what these tokens are)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;GPT-2 does not use an &lt;code&gt;&amp;lt;UNK&amp;gt;&lt;/code&gt; token for out-of-vocabulary words; instead, GPT-2 uses a byte-pair encoding (BPE) tokenizer, which breaks down words into subword units which we will discuss in a later section&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We use the &lt;code&gt;&amp;lt;|endoftext|&amp;gt;&lt;/code&gt; tokens between two independent sources of text:
&lt;img src=&#34;https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/10.webp&#34; width=&#34;500px&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Let&amp;rsquo;s see what happens if we tokenize the following text:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The above produces an error because the word &amp;ldquo;Hello&amp;rdquo; is not contained in the vocabulary&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;To deal with such cases, we can add special tokens like &lt;code&gt;&amp;quot;&amp;lt;|unk|&amp;gt;&amp;quot;&lt;/code&gt; to the vocabulary to represent unknown words&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Since we are already extending the vocabulary, let&amp;rsquo;s add another token called &lt;code&gt;&amp;quot;&amp;lt;|endoftext|&amp;gt;&amp;quot;&lt;/code&gt; which is used in GPT-2 training to denote the end of a text (and it&amp;rsquo;s also used between concatenated text, like if our training datasets consists of multiple articles, books, etc.)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We also need to adjust the tokenizer accordingly so that it knows when and how to use the new &lt;code&gt;&amp;lt;unk&amp;gt;&lt;/code&gt; token
Let&amp;rsquo;s try to tokenize text with the modified tokenizer:&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;25-bytepair-encoding&#34;&gt;2.5 BytePair encoding
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;GPT-2 used BytePair encoding (BPE) as its tokenizer&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;it allows the model to break down words that aren&amp;rsquo;t in its predefined vocabulary into smaller subword units or even individual characters, enabling it to handle out-of-vocabulary words&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For instance, if GPT-2&amp;rsquo;s vocabulary doesn&amp;rsquo;t have the word &amp;ldquo;unfamiliarword,&amp;rdquo; it might tokenize it as [&amp;ldquo;unfam&amp;rdquo;, &amp;ldquo;iliar&amp;rdquo;, &amp;ldquo;word&amp;rdquo;] or some other subword breakdown, depending on its trained BPE merges&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The original BPE tokenizer can be found here: &lt;a class=&#34;link&#34; href=&#34;https://github.com/openai/gpt-2/blob/master/src/encoder.py&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/openai/gpt-2/blob/master/src/encoder.py&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In this chapter, we are using the BPE tokenizer from OpenAI&amp;rsquo;s open-source &lt;a class=&#34;link&#34; href=&#34;https://github.com/openai/tiktoken&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;tiktoken&lt;/a&gt; library, which implements its core algorithms in Rust to improve computational performance&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;I created a notebook in the &lt;a class=&#34;link&#34; href=&#34;../02_bonus_bytepair-encoder&#34; &gt;./bytepair_encoder&lt;/a&gt; that compares these two implementations side-by-side (tiktoken was about 5x faster on the sample text)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;BPE tokenizers break down unknown words into subwords and individual characters:
&lt;img src=&#34;https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/11.webp&#34; width=&#34;300px&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;26-data-sampling-with-a-sliding-window&#34;&gt;2.6 Data sampling with a sliding window
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;We train LLMs to generate one word at a time, so we want to prepare the training data accordingly where the next word in a sequence represents the target to predict:
&lt;img src=&#34;https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/12.webp&#34; width=&#34;400px&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For each text chunk, we want the inputs and targets&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Since we want the model to predict the next word, the targets are the inputs shifted by one position to the right&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;One by one, the prediction would look like as follows:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We will take care of the next-word prediction in a later chapter after we covered the attention mechanism&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For now, we implement a simple data loader that iterates over the input dataset and returns the inputs and targets shifted by one&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Install and import PyTorch (see Appendix A for installation tips)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We use a sliding window approach, changing the position by +1:&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/13.webp?123&#34; width=&#34;500px&#34;&gt;
- Create dataset and dataloader that extract chunks from the input text dataset
- Let&#39;s test the dataloader with a batch size of 1 for an LLM with a context size of 4:
- An example using stride equal to the context length (here: 4) as shown below:
&lt;img src=&#34;https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/14.webp&#34; width=&#34;500px&#34;&gt;
- We can also create batched outputs
&lt;ul&gt;
&lt;li&gt;Note that we increase the stride here so that we don&amp;rsquo;t have overlaps between the batches, since more overlap could lead to increased overfitting&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;27-creating-token-embeddings&#34;&gt;2.7 Creating token embeddings
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The data is already almost ready for an LLM&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;But lastly let us embed the tokens in a continuous vector representation using an embedding layer&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Usually, these embedding layers are part of the LLM itself and are updated (trained) during model training
&lt;img src=&#34;https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/15.webp&#34; width=&#34;400px&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Suppose we have the following four input examples with input ids 2, 3, 5, and 1 (after tokenization):&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For the sake of simplicity, suppose we have a small vocabulary of only 6 words and we want to create embeddings of size 3:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;This would result in a 6x3 weight matrix:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For those who are familiar with one-hot encoding, the embedding layer approach above is essentially just a more efficient way of implementing one-hot encoding followed by matrix multiplication in a fully-connected layer, which is described in the supplementary code in &lt;a class=&#34;link&#34; href=&#34;../03_bonus_embedding-vs-matmul&#34; &gt;./embedding_vs_matmul&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Because the embedding layer is just a more efficient implementation that is equivalent to the one-hot encoding and matrix-multiplication approach it can be seen as a neural network layer that can be optimized via backpropagation&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;To convert a token with id 3 into a 3-dimensional vector, we do the following:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Note that the above is the 4th row in the &lt;code&gt;embedding_layer&lt;/code&gt; weight matrix&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;To embed all four &lt;code&gt;input_ids&lt;/code&gt; values above, we do&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;An embedding layer is essentially a look-up operation:
&lt;img src=&#34;https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/16.webp?123&#34; width=&#34;500px&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;You may be interested in the bonus content comparing embedding layers with regular linear layers: &lt;a class=&#34;link&#34; href=&#34;../03_bonus_embedding-vs-matmul&#34; &gt;../03_bonus_embedding-vs-matmul&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;28-encoding-word-positions&#34;&gt;2.8 Encoding word positions
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Embedding layer convert IDs into identical vector representations regardless of where they are located in the input sequence:
&lt;img src=&#34;https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/17.webp&#34; width=&#34;400px&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Positional embeddings are combined with the token embedding vector to form the input embeddings for a large language model:
&lt;img src=&#34;https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/18.webp&#34; width=&#34;500px&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The BytePair encoder has a vocabulary size of 50,257:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Suppose we want to encode the input tokens into a 256-dimensional vector representation:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If we sample data from the dataloader, we embed the tokens in each batch into a 256-dimensional vector&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If we have a batch size of 8 with 4 tokens each, this results in a 8 x 4 x 256 tensor:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;GPT-2 uses absolute position embeddings, so we just create another embedding layer:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;To create the input embeddings used in an LLM, we simply add the token and the positional embeddings:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In the initial phase of the input processing workflow, the input text is segmented into separate tokens&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Following this segmentation, these tokens are transformed into token IDs based on a predefined vocabulary:
&lt;img src=&#34;https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/19.webp&#34; width=&#34;400px&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;summary-and-takeaways&#34;&gt;Summary and takeaways
&lt;/h1&gt;&lt;p&gt;See the &lt;a class=&#34;link&#34; href=&#34;./dataloader.ipynb&#34; &gt;./dataloader.ipynb&lt;/a&gt; code notebook, which is a concise version of the data loader that we implemented in this chapter and will need for training the GPT model in upcoming chapters.&lt;/p&gt;
&lt;p&gt;See &lt;a class=&#34;link&#34; href=&#34;./exercise-solutions.ipynb&#34; &gt;./exercise-solutions.ipynb&lt;/a&gt; for the exercise solutions.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Archives</title>
        <link>https://CarlossQAQ.github.io/archives/</link>
        <pubDate>Tue, 28 May 2019 00:00:00 +0000</pubDate>
        
        <guid>https://CarlossQAQ.github.io/archives/</guid>
        <description></description>
        </item>
        <item>
        <title>Math Typesetting</title>
        <link>https://CarlossQAQ.github.io/p/math-typesetting/</link>
        <pubDate>Fri, 08 Mar 2019 00:00:00 +0000</pubDate>
        
        <guid>https://CarlossQAQ.github.io/p/math-typesetting/</guid>
        <description>&lt;p&gt;Mathematical notation in a Hugo project can be enabled by using third party JavaScript libraries.&lt;/p&gt;
&lt;p&gt;In this example we will be using &lt;a class=&#34;link&#34; href=&#34;https://katex.org/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;KaTeX&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create a partial under &lt;code&gt;/layouts/partials/math.html&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Within this partial reference the &lt;a class=&#34;link&#34; href=&#34;https://katex.org/docs/autorender.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Auto-render Extension&lt;/a&gt; or host these scripts locally.&lt;/li&gt;
&lt;li&gt;Include the partial in your templates like so:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;{{&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; or .Params.math .Site.Params.math &lt;span class=&#34;o&#34;&gt;}}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;{{&lt;/span&gt; partial &lt;span class=&#34;s2&#34;&gt;&amp;#34;math.html&amp;#34;&lt;/span&gt; . &lt;span class=&#34;o&#34;&gt;}}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;{{&lt;/span&gt; end &lt;span class=&#34;o&#34;&gt;}}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;To enable KaTeX globally set the parameter &lt;code&gt;math&lt;/code&gt; to &lt;code&gt;true&lt;/code&gt; in a project&amp;rsquo;s configuration&lt;/li&gt;
&lt;li&gt;To enable KaTeX on a per page basis include the parameter &lt;code&gt;math: true&lt;/code&gt; in content files&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Use the online reference of &lt;a class=&#34;link&#34; href=&#34;https://katex.org/docs/supported.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Supported TeX Functions&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;examples&#34;&gt;Examples
&lt;/h3&gt;&lt;p&gt;Inline math: $\varphi = \dfrac{1+\sqrt5}{2}= 1.6180339887…$&lt;/p&gt;
&lt;p&gt;Block math:
$$
\varphi = 1+\frac{1} {1+\frac{1} {1+\frac{1} {1+\cdots} } }
$$&lt;/p&gt;</description>
        </item>
        <item>
        <title>About</title>
        <link>https://CarlossQAQ.github.io/about/</link>
        <pubDate>Thu, 28 Feb 2019 00:00:00 +0000</pubDate>
        
        <guid>https://CarlossQAQ.github.io/about/</guid>
        <description>&lt;p&gt;Written in Go, Hugo is an open source static site generator available under the &lt;a class=&#34;link&#34; href=&#34;https://github.com/gohugoio/hugo/blob/master/LICENSE&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Apache License 2.0.&lt;/a&gt; Hugo supports TOML, YAML and JSON data file types, Markdown and HTML content files and uses shortcodes to add rich content. Other notable features are taxonomies, multilingual mode, image processing, custom output formats, HTML/CSS/JS minification and support for Sass SCSS workflows.&lt;/p&gt;
&lt;p&gt;Hugo makes use of a variety of open source projects including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/yuin/goldmark&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/yuin/goldmark&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/alecthomas/chroma&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/alecthomas/chroma&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/muesli/smartcrop&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/muesli/smartcrop&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/spf13/cobra&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/spf13/cobra&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/spf13/viper&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/spf13/viper&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Hugo is ideal for blogs, corporate websites, creative portfolios, online magazines, single page applications or even a website with thousands of pages.&lt;/p&gt;
&lt;p&gt;Hugo is for people who want to hand code their own website without worrying about setting up complicated runtimes, dependencies and databases.&lt;/p&gt;
&lt;p&gt;Websites built with Hugo are extremely fast, secure and can be deployed anywhere including, AWS, GitHub Pages, Heroku, Netlify and any other hosting provider.&lt;/p&gt;
&lt;p&gt;Learn more and contribute on &lt;a class=&#34;link&#34; href=&#34;https://github.com/gohugoio&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Links</title>
        <link>https://CarlossQAQ.github.io/links/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://CarlossQAQ.github.io/links/</guid>
        <description>&lt;p&gt;To use this feature, add &lt;code&gt;links&lt;/code&gt; section to frontmatter.&lt;/p&gt;
&lt;p&gt;This page&amp;rsquo;s frontmatter:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;9
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nt&#34;&gt;links&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;title&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;GitHub&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;description&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;GitHub is the world&amp;#39;s largest software development platform.&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;website&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;https://github.com&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;image&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;title&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;TypeScript&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;description&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;TypeScript is a typed superset of JavaScript that compiles to plain JavaScript.&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;website&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;https://www.typescriptlang.org&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;image&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;ts-logo-128.jpg&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;code&gt;image&lt;/code&gt; field accepts both local and external images.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Search</title>
        <link>https://CarlossQAQ.github.io/search/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://CarlossQAQ.github.io/search/</guid>
        <description></description>
        </item>
        
    </channel>
</rss>
